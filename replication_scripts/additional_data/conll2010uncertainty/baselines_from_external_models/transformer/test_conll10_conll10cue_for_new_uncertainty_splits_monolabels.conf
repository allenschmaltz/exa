[config]
experiment_name = final_soft_attention
dataset = conll10
dataset_split = test
method = soft_attention
model_name = roberta-base
datetime = 20210620T223557
results_input_dir = UPDATE_WITH_YOUR_PATH/token_preds/{method}/{experiment_name}/{model_name}/{dataset_name}/{datetime}/
results_input_filename = token_scores.tsv
importance_threshold = 0.50
top_count = 0
preds_output_filename = token_label_pred.tsv
eval_results_filaname = token_results.txt
max_seq_length = 128
per_device_eval_batch_size = 64
seed = 15
test_label_dummy = test
lowercase = True
do_mask_words = False
mask_prob = 0.0
hid_to_attn_dropout = 0.10
attention_evidence_size = 100
final_hidden_layer_size = 300
initializer_name = glorot
attention_activation = soft
soft_attention = False
soft_attention_gamma = 0.1
